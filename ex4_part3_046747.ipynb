{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNxy0Tqnfo83yJBQ7PEw5eJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomer9080/DL-Speech-exercises/blob/main/ex4_part3_046747.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3 - Using x-vectors as embeddings for classification tasks"
      ],
      "metadata": {
        "id": "l5YO-JcjpDIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, you will learn how to use speech embedding models to perform speech classification tasks.\n",
        "\n",
        "You will be working in a Google Colab environment.\n",
        "\n",
        "A few important notes:\n",
        "\n",
        "* Make sure to copy this notebook to your own drive so your progress is saved.\n",
        "* Change the runtime type to GPU to achieve faster inference with our embedding model. (To do this, click on 'Runtime' → 'Change runtime type' in the toolbar.)\n",
        "* You can add any necessary imports (e.g., when selecting classifiers) across different code cells."
      ],
      "metadata": {
        "id": "1IFS2pLmEoIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "8pabuc_opKRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install speechbrain"
      ],
      "metadata": {
        "id": "0TSTOOITVSZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KSkffGNUq3Q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import random\n",
        "import librosa\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.model_selection import KFold\n",
        "from speechbrain.inference.classifiers import EncoderClassifier\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 - SPEECHCOMMANDS Dataset"
      ],
      "metadata": {
        "id": "ZId5coe-p1_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SPEECHCOMMANDS dataset consists of approximately 2,000 speakers and around 100,000 utterances.\n",
        "\n",
        "The code below downloads both the training and validation splits."
      ],
      "metadata": {
        "id": "K-4IiwI6p7Qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DS loading - takes aprroximately 2-3 mins, be patient.\n",
        "dataset_train = torchaudio.datasets.SPEECHCOMMANDS(\n",
        "    root=\"./\",\n",
        "    download=True,\n",
        "    subset=\"training\"\n",
        ")"
      ],
      "metadata": {
        "id": "2qcKFADFWn2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below loads the embedding model to the device, and sets model mode to evaluation."
      ],
      "metadata": {
        "id": "6G1v6IxXwCn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Given code\n",
        "xvector_model = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"./\")\n",
        "xvector_model.device = device\n",
        "xvector_model.to(device)\n",
        "xvector_model.eval()"
      ],
      "metadata": {
        "id": "Chhn7YPDaMaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 - Speaker Classification"
      ],
      "metadata": {
        "id": "b0JbexrlpQaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each sample on the file tree of SPEECHCOMMANDS includes a unique speaker id.\n",
        "\n",
        "The below code extracts all the speaker ids in the dataset to `spkr_ids` list."
      ],
      "metadata": {
        "id": "jdK54q7HraE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Given code\n",
        "wav_paths = [os.path.join(root, file) for root, dirs, files in os.walk('./SpeechCommands/speech_commands_v0.02') for file in files if file.endswith('.wav')]\n",
        "spkr_ids = list({os.path.basename(wav_path).split('_')[0] for wav_path in wav_paths})"
      ],
      "metadata": {
        "id": "nwWmsVEsaCIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.1 - t-SNE Projection"
      ],
      "metadata": {
        "id": "ZjIO5yDVshJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "t-SNE projection is similar to PCA. We will use it to visualize how our embedding model maps the speech files onto a space relevant to our task.\n",
        "\n",
        "You can read more about t-SNE here:\n",
        "https://scikit-learn.org/stable/modules/manifold.html#t-distributed-stochastic-neighbor-embedding-t-sne\n",
        "\n",
        "And about sklearn's TSNE implementation here:\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n",
        "\n",
        "There is no need to dive deeply into the implementation or technical details of t-SNE, but feel free to explore further and broaden your horizons. :)"
      ],
      "metadata": {
        "id": "A3GeWfDyvi1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, you will gain an understanding of how x-vector embeddings map speech data onto another latent space.\n",
        "\n",
        "Follow these instructions:\n",
        "\n",
        "1. Select 10 speakers from the spkr_ids list. Choose speakers with more than 100 utterances, and ensure all selected speakers have a similar number of samples.\n",
        "1. For the chosen speakers, create a list of all wav files associated with them (across all different classes).\n",
        "1. Ensure your list contains at least 1,000 wav files, with an even distribution of samples across speakers.\n",
        "1. For each wav file in the list, extract its embedding and the corresponding spkr_id label.\n",
        "1. Generate a scatter plot showing the t-SNE projection of the extracted embeddings. Assign a unique color to each speaker's data points on the scatter plot.\n",
        "\n",
        "**Hint**: Use two lists while generating the embeddings—one for the embeddings and another for the speaker IDs.\n",
        "\n",
        "**Notes**:\n",
        "* Use a 2-dimensional t-SNE projection.\n",
        "* Include an appropriate title and labels for the axes in the plot."
      ],
      "metadata": {
        "id": "rYgXpzU2stic"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ibZiGVRK4fO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.2 - Classify Speakers"
      ],
      "metadata": {
        "id": "h6hKOJQSpCTu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On this section, you will classify different speakers using embeddings from our embedding model.\n",
        "\n",
        "1. Collect wav files of **20** different speakers. (Follow the same guidelines from 3.2.1.1).\n",
        "1. Choose three different classifiers from `sklearn` library (e.g., SVM, Decision Tree, Random Forest, LDA, etc.)\n",
        "1. Add a table of the results, using 5-Fold Crossv-Validation (CV) as specified in the exercise PDF. Discuss the results.\n",
        "\n",
        "**Note**: Present mean and standard-deviation of classification results per each classifier.\n"
      ],
      "metadata": {
        "id": "_ksPOPbrxMe9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dPey7pBhK76-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 - Command Classification"
      ],
      "metadata": {
        "id": "Ocscr_cl0mz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SPEECHCOMMANDS dataset was originally created to train speech models for classifying different commands.\n",
        "\n",
        "In this section, we will evaluate our embedding model on a command classification task."
      ],
      "metadata": {
        "id": "hS0jY3xCH6h5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.1 - t-SNE Projection\n",
        "\n"
      ],
      "metadata": {
        "id": "QzRlYWXK0rQV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeat the process from Section 3.2.1, but this time use the commands as labels in the t-SNE projection.\n",
        "\n",
        "1. Select only **three** commands from the following set: {left, happy, marvin, go, zero, right}.\n",
        "1. Randomly select 1,000 samples for each chosen command.\n",
        "1. This should result in a dataset of 3,000 samples, divided into three classes (the three commands you chose), with 1,000 samples per command.\n",
        "1. Generate a scatter plot of the t-SNE projection and discuss the results in your report."
      ],
      "metadata": {
        "id": "oDLcr1AE1VVN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XeKd9rXxyBSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.2 - Classify Commands\n",
        "\n",
        "Repeat the process from Section 3.2.2, but this time use commands as labels for classification.\n",
        "\n",
        "1. Use the embeddings from Section 3.3.1 to classify the different classes.\n",
        "1. Display a table of the classification results and discuss your findings in the report.\n",
        "\n",
        "\n",
        "**Note**: Present the classification results using 5-fold cross-validation (CV).\n",
        "\n",
        "**Note**: Present mean and standard-deviation of classification results per each classifier."
      ],
      "metadata": {
        "id": "cfyRvXuf1_lt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZchlN-fVzB13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Good Luck!"
      ],
      "metadata": {
        "id": "Q0MX56XrKiQ5"
      }
    }
  ]
}