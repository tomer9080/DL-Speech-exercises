{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomer9080/DL-Speech-exercises/blob/main/amdf_lpc_046747_ex2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3em7rKxQpAX"
      },
      "source": [
        "# Pitch estimation using AMDF, Linear Predictive Coding, LPC Spectrum Processing\n",
        "\n",
        "In this exercise, you will deploy an end-to-end pipeline, that starts from recording an utterance. The next step is to analyze the acoustical properties of the recorded waveform, such as pitch and formant extraction and classification of voiced/unvoiced segments. To complete the task, you will implement the algorithm to find the LPC Coefficients, then process the LPC Spectrum in order to examine the effect of warping the LPC spectrum.\n",
        "\n",
        "> You might use Google-Colab to complete the exercise, or use a local setup. If you choose to use local setup, make sure all required packages are installed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjan4CWhYiR-"
      },
      "source": [
        "## Report Guidelines for Plots and Speech Signal Figures\n",
        "\n",
        "Follow these guidelines to ensure your report is clear, professional, and easy to evaluate:\n",
        "\n",
        "### 1. Figures and Captions\n",
        "- Every figure must have a **descriptive caption**.\n",
        "- Ensure figures are readable (clear fonts, non-pixelated, proper line thickness).\n",
        "\n",
        "### 2. Axes and Labels\n",
        "- Always label axes with **names and units** (e.g., Time (s), Frequency (Hz)).\n",
        "- Use consistent axis limits and scales when comparing signals.\n",
        "\n",
        "### 3. Interpretation\n",
        "- Do not just show plotsâ€”**explain** what they mean.\n",
        "- Highlight key observations (peaks, trends, transitions).\n",
        "- Compare your results with ground truth or theory when relevant.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki_49jlrRoW2"
      },
      "source": [
        "## ðŸ Import python libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fg9lsLa9RrU5"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "import scipy\n",
        "import librosa\n",
        "import datasets\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib widget"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYV2ZK_dSBQ9"
      },
      "source": [
        "## ðŸŽ¤ Loading the utterance\n",
        "\n",
        "Load the file attached in the moodle. Note that are 2 files: a `.wav` file and a `.phn` file.\n",
        "\n",
        "The sound file contains the utterance:\n",
        "\n",
        "> *She had your dark suit in greasy wash water all year.*\n",
        "\n",
        "The `.phn` file contains the mapping between samples and the phone said in the `.wav` file.\n",
        "\n",
        "Load the sound file using the code below and play it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HjRIPtBQkcc"
      },
      "outputs": [],
      "source": [
        "## Do not remove\n",
        "import IPython.display as ipd\n",
        "\n",
        "# Load the two recordings\n",
        "y1, sr1 = librosa.load('sa1.wav', sr=16000)\n",
        "\n",
        "# Play the first recording\n",
        "print(\"Playing utterance:\")\n",
        "display(ipd.Audio(y1, rate=sr1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vw9PHLTFX5Da"
      },
      "source": [
        "Plot your recording signals using the code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KqFV2fIZITi"
      },
      "outputs": [],
      "source": [
        "## Do not remove\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 1, 1)\n",
        "plt.plot(np.arange(len(y1)) / sr1, y1)\n",
        "plt.title('Recording 1')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOSMb7CNGWEz"
      },
      "source": [
        "Now, run the below code to get a list that resembles the sequence of phones in the utterance.\n",
        "\n",
        "Each entry in the list has a `phone` field, `start` field and `end` field. Both `start, end` units are samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDkfvDkuGWEz"
      },
      "outputs": [],
      "source": [
        "## Do not remove\n",
        "with open(\"sa1.phn\", \"r\") as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "phones = []\n",
        "for line in lines:\n",
        "    start, end, phone = line.strip().split()\n",
        "    phones.append({\"start\": int(start), \"end\": int(end), \"phone\": phone})\n",
        "\n",
        "print(f\"{phones=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJyNEGlnXL-6"
      },
      "source": [
        "## 1. Pitch Estimation using the Average Magnitude Difference Function (AMDF)\n",
        "\n",
        "In this section, you will learn what the Average Magnitude Difference Function (AMDF) is and how it can be used to estimate the pitch (fundamental frequency) of a speech signal.\n",
        "\n",
        "The AMDF is computed over short-time frames and is defined as:\n",
        "\n",
        "$$\n",
        "D(\\tau) = \\frac{1}{N} \\sum_{n=0}^{N-1} |x(n) - x(n+\\tau)|\n",
        "$$\n",
        "\n",
        "Here, $x(n)$ represents the speech samples, $N$ is the analysis frame length, and $\\tau$ is the time lag.\n",
        "\n",
        "For a periodic signal with period $T_0$, the AMDF typically exhibits a clear minimum when $\\tau = T_0$. Therefore, the pitch period can be estimated by selecting the lag value that minimizes the AMDF:\n",
        "\n",
        "$$\n",
        "T_0 = \\arg\\min_{\\tau} D(\\tau), \\quad \\tau \\in [\\tau_{\\min}, \\tau_{\\max}]\n",
        "$$\n",
        "\n",
        "The values $\\tau_{\\min}$ and $\\tau_{\\max}$ should be chosen according to the expected pitch range.\n",
        "\n",
        "In this task, we limit the pitch search range to frequencies between **50 Hz and 400 Hz**, which is a suitable range for typical speech signals. When implementing the method, this frequency range must be converted into corresponding lag values based on the sampling rate.\n",
        "\n",
        "> **Note**: When choosing an interval of speech to evaluate the pitch frequency on, make sure you include at least 5 pitch cycles.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNrEGnClYBxC"
      },
      "source": [
        "### 1.1 Vanilla AMDF\n",
        "\n",
        "In this part, you will apply the pitch estimation method described in Section 1 to your recorded utterances.\n",
        "\n",
        "Begin by displaying the waveform of each student's utterance. From each utterance, select **one periodic segment** (voiced speech) and **one non-periodic segment** (unvoiced or noise-like speech). You can choose the segments by using the `phones` list provided earlier.\n",
        "In the report, clearly specify:\n",
        "- which segments you selected,\n",
        "- the sample indices (boundaries) of the segments you will work on.\n",
        "\n",
        "For each selected segment, compute and plot the AMDF function $D(\\tau)$ alongside the corresponding time-domain signal.\n",
        "\n",
        "The AMDF is computed as:\n",
        "$$\n",
        "D(\\tau) = \\frac{1}{N} \\sum_{n=0}^{N-1} |x(n) - x(n+\\tau)|\n",
        "$$\n",
        "\n",
        "For a periodic segment, $D(\\tau)$ is expected to show a strong minimum at the pitch period $T_0$. In your plots, mark the estimated pitch period $T_0$ by identifying:\n",
        "$$\n",
        "T_0 = \\arg\\min_{\\tau} D(\\tau)\n",
        "$$\n",
        "\n",
        "For the non-periodic segment, you should observe that $D(\\tau)$ does **not** exhibit a clear minimum, reflecting the lack of periodic structure.\n",
        "\n",
        "Be sure to clearly annotate all plots, indicating:\n",
        "- the waveform segment,\n",
        "- the AMDF curve $D(\\tau)$,\n",
        "- and, for periodic segments, the estimated pitch period $T_0$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqBmnoyklx8b"
      },
      "outputs": [],
      "source": [
        "## Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQmLQHz8lyTe"
      },
      "source": [
        "### 1.2 Circular AMDF\n",
        "\n",
        "In this part, you will apply a modified version of the AMDF method by using its circular form.  \n",
        "Use the same periodic and non-periodic segments selected in Section 1.1.\n",
        "\n",
        "For each selected segment, compute and plot the Circular AMDF (CAMDF) function $D_c(\\tau)$ alongside the corresponding time-domain signal.\n",
        "\n",
        "The CAMDF is defined as:\n",
        "$$\n",
        "D_c(\\tau) = \\frac{1}{N} \\sum_{n=0}^{N-1} \\big| x((n+\\tau) \\bmod N) - x(n) \\big|\n",
        "$$\n",
        "\n",
        "This formulation treats the signal segment as if it were **circularly periodic** with length $N$.  \n",
        "Under this circularity assumption, the CAMDF satisfies the symmetry property:\n",
        "$$\n",
        "D_c(\\tau) = D_c(N - \\tau),\n",
        "$$\n",
        "which means that only the range $\\tau \\in [0, \\frac{N}{2}]$ needs to be evaluated.\n",
        "\n",
        "**Important considerations:**\n",
        "\n",
        "- The symmetry property holds because of the *circular indexing*, not because the speech signal itself is symmetric.  \n",
        "- Circular indexing may introduce artificial structure if the endpoints of the segment do not match.  \n",
        "  Therefore, choose segment lengths that contain at least **3 pitch periods** to avoid misleading minima.\n",
        "- Even for non-periodic segments, CAMDF may produce weak or spurious minima due to circular wrap-around.  \n",
        "  You should interpret such minima cautiously.\n",
        "\n",
        "For periodic segments, $D_c(\\tau)$ is expected to exhibit a clear minimum at the pitch period $T_0$. Mark this point on your plots by identifying:\n",
        "$$\n",
        "T_0 = \\arg\\min_{\\tau} D_c(\\tau)\n",
        "$$\n",
        "\n",
        "For non-periodic segments, the CAMDF should not show a stable or prominent minimum, although small artificial minima may appear because of the circular structure.\n",
        "\n",
        "In your report, include for each segment:\n",
        "- the waveform segment,\n",
        "- the CAMDF curve $D_c(\\tau)$ over $\\tau \\in [0, \\frac{N}{2}]$,\n",
        "- and, for periodic segments, the estimated pitch period $T_0$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HquQv9ZXGWE0"
      },
      "outputs": [],
      "source": [
        "## Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iohQB6RIrYvy"
      },
      "source": [
        "### 1.3 Comparison with Built-In Python Pitch Estimation\n",
        "\n",
        "In this part, use the `librosa.yin` function to compute the pitch frequency for the periodic segments selected earlier. Compare the pitch values obtained from the built-in YIN algorithm with the estimates you obtained using the AMDF (Section 1.1) and Circular AMDF (Section 1.2).\n",
        "\n",
        "In your report, clearly list the pitch estimates from all three methods:\n",
        "- AMDF (Section 1.1)\n",
        "- Circular AMDF (Section 1.2)\n",
        "- `librosa.yin`\n",
        "\n",
        "> **Note**: To retrieve the pitch value estimated from the YIN algorithm, apply `np.median` on `librosa.yin` result.\n",
        "\n",
        "You should use this comparison as a sanity check to the correctness of your AMDF estimations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxdfjKj6GWE0"
      },
      "outputs": [],
      "source": [
        "## Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4I4qfx-p37-4"
      },
      "source": [
        "## 2. LPC spectrum and Formants Extraction\n",
        "\n",
        "In this section, you will use the LPC spectrum to extract formants from speech utterances.\n",
        "\n",
        "You will work on a dataset called *Hillenbrand*, which contains vowels utterances of children and adults (male and female). More specifically, the dataset contains words of the form */h-d/* where \"-\" is replaced with a different vowel.\n",
        "\n",
        "Using this dataset, you will extract the formants for each vowel per each group (boys, girls, men, women) using the LPC spectrum and plot it on a scatter plot.\n",
        "\n",
        "You will compare your extracted formants with the manually annotated formants in the dataset.\n",
        "\n",
        "Load the dataset using the code provided below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdtsSq-khjcs"
      },
      "outputs": [],
      "source": [
        "## Do not remove\n",
        "import ast\n",
        "import datasets\n",
        "\n",
        "ds = datasets.load_dataset(\"MLSpeech/hillenbrand_vowels\", split=\"train\")\n",
        "\n",
        "def cast_all(example):\n",
        "    for key in [\"audio\", \"formant_1\", \"formant_2\", \"formant_3\", \"formant_4\"]:\n",
        "        if key in example:\n",
        "            example[key] = np.array(ast.literal_eval(example[key]))\n",
        "    return example\n",
        "\n",
        "ds = ds.map(cast_all)\n",
        "\n",
        "# sample keys\n",
        "print(f\"{ds[0].keys()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JxGpLF25lEu"
      },
      "source": [
        "### 2.1 The LPC Spectrum\n",
        "\n",
        "In order to extract the speech formants, we will use the LPC spectrum. As seen in class, the LPC spectrum yields a smoother spectral representation than the standard FFT spectrum. This smoothness allows us to better observe the formants and the pitch frequency of the speech signal.\n",
        "\n",
        "In the code below, implement the function `get_lpc_spectrum`.\n",
        "\n",
        "The function should:\n",
        "\n",
        "- Take a speech signal `x` and a filter `order` as input.\n",
        "- Compute LPC coefficients using `librosa.lpc`.\n",
        "- Compute the LPC spectrum using `scipy.signal.freqz`.\n",
        "- Return both the frequency vector and the LPC spectrum magnitude (in dB).\n",
        "\n",
        "> **Reminder**: $\\mathrm{dB}(x) = 20\\log_{10}(|x|)$. It is a good practice to add a small quantity, $\\epsilon=10^{-12}$ to the $\\log$ argument for numerical stability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enkw78mmDtYs"
      },
      "outputs": [],
      "source": [
        "from scipy.signal import freqz\n",
        "\n",
        "def get_lpc_spectrum(x, sr, order):\n",
        "    \"\"\"\n",
        "    Computes the LPC spectrum of a speech signal.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : np.ndarray\n",
        "        Input speech signal.\n",
        "    sr : int\n",
        "        Sampling rate of the signal.\n",
        "    order : int\n",
        "        LPC filter order.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    freqs : np.ndarray\n",
        "        Frequency vector (Hz).\n",
        "    lpc_spectrum_db : np.ndarray\n",
        "        LPC spectrum magnitude in dB.\n",
        "    \"\"\"\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DAGkdVzD60N"
      },
      "source": [
        "* Select a single sample from the Hillenbrand dataset and plot its LPC spectrum. Include the plot in your report. Then, try to estimate the formants approximately by visually inspecting the LPC spectrum. Add your estimations to the report.\n",
        "\n",
        "> **Note:** When computing the LPC spectrum of a vowel, use the steady-state portion at the center of the vowelâ€”typically a 40â€¯ms segment from the middle of the recording.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73fP-ZScEHBq"
      },
      "outputs": [],
      "source": [
        "## Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5nIABAMGJia"
      },
      "source": [
        "### 2.2 Extracting Formants\n",
        "\n",
        "Once we have the LPC spectrum, we can estimate the formants by finding the **peaks** in the spectrum. Formants typically correspond to the **local maxima** of the LPC spectrum.  \n",
        "\n",
        "In Python, you can use `scipy.signal.find_peaks` to detect these maxima. You may want to keep only the first few formants (F0, F1, F2, F3) which are the most relevant for vowel analysis.\n",
        "\n",
        "Implement the function `extract_formants` as described below.\n",
        "\n",
        "The function should:\n",
        "\n",
        "* Take the LPC spectrum and corresponding frequency vector as input.\n",
        "* Find the peaks in the LPC spectrum using `find_peaks`.\n",
        "* Return the first 4 formants (F0, F1, F2, F3) in Hz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ddi4PboGG0xZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "def extract_formants(freqs, lpc_spectrum):\n",
        "    \"\"\"\n",
        "    Extract the first four formant frequencies (F0-F3) from an LPC spectrum.\n",
        "\n",
        "    Args:\n",
        "        freqs (np.ndarray): Frequency axis (Hz).\n",
        "        lpc_spectrum (np.ndarray): LPC magnitude spectrum.\n",
        "\n",
        "    Returns:\n",
        "        list: [F0, F1, F2, F3] in Hz (if not enough peaks are found - default is -1).\n",
        "    \"\"\"\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjTyu5hfHbbE"
      },
      "source": [
        "Now use the function `extract_formants` to compare the formants resulted from the function to the ones you approximated in section 2.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZNUMosFHsrl"
      },
      "outputs": [],
      "source": [
        "## Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH5YYzg4Hvbm"
      },
      "source": [
        "### 2.3 Extract Formants from the Dataset\n",
        "\n",
        "In this section, we use the Hillenbrand dataset, which contains vowel recordings categorized into four speaker groups: boys, girls, men, and women. The filenames indicate the speaker group (e.g., recordings from boys start with \"b\", from girls with \"g\", etc.).\n",
        "\n",
        "Iterate through all `wav` files in the dataset and extract the formants for each file. Each extracted result should be organized by **group** and **vowel** (for example: *group: girl, vowel: ae*) to allow for later analysis.  \n",
        "When computing the LPC spectrum, ensure that you use only the **vowel core**â€”a 40â€“50 ms segment taken from the center of each vowel recording.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nF4dLT3eKZkC"
      },
      "outputs": [],
      "source": [
        "## Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUqHpLaEKkCV"
      },
      "source": [
        "Now, compare the formants you estimated with the annotated results provided in the Hillenbrand dataset. For this task, focus only on **F1** and **F2**.\n",
        "\n",
        "Each `.wav` file in the dataset has a corresponding formant array, where each element represents a 10ms frame of the speech signal. To compare your estimates with the ground truth:\n",
        "\n",
        "1. Make sure to extract the correct time frames from the dataset formant arrays that correspond to the segment you analyzed.\n",
        "2. Average the formants along the time axis to obtain a single value for **F1** and **F2**.\n",
        "\n",
        "At this point, you should have **two sets of formants**: one from your LPC-based functions and one from the dataset annotations.\n",
        "\n",
        "Next:\n",
        "\n",
        "* Create a scatter plot of vowels in the **F1â€“F2 space**.\n",
        "* Separate the plots by speaker groups: **boys, girls, women, men**.\n",
        "* Present **two subfigures side by side**: one showing your estimated formants and one showing the dataset formants, per group (i.e, each sub-figure will contain scatter of vowels formants values per group, per each group. In total 4 groups of scatter plots on each sub figure). Make sure you average each vowel formants values over the group axis before plotting the scatter plots.\n",
        "* Analyze and discuss the differences between your estimates and the annotated dataset values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLgJ_f9qYQn-"
      },
      "outputs": [],
      "source": [
        "## Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuarWwyhji_5"
      },
      "source": [
        "# Good Luck!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "proj_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}